{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cb8c2546",
   "metadata": {},
   "source": [
    "# ðŸš€ PCM-LLM: Prompt Compression Benchmark\n",
    "\n",
    "A comprehensive framework for testing and evaluating prompt compression methods for Large Language Models (LLMs).\n",
    "\n",
    "**Optimized for Google Colab Free Tier**\n",
    "- âœ… 4-bit quantization for maximum memory efficiency\n",
    "- âœ… Memory monitoring and automatic cleanup\n",
    "- âœ… Optimized for limited GPU memory (15GB)\n",
    "- âœ… Fast execution with small batch sizes\n",
    "\n",
    "---\n",
    "\n",
    "## Quick Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e6ba7ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clone the repository\n",
    "!git clone https://github.com/yourusername/pcm-llm.git\n",
    "%cd pcm-llm\n",
    "\n",
    "# Install dependencies\n",
    "!pip install -q -r requirements.txt\n",
    "\n",
    "# Check GPU availability\n",
    "import torch\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.1f} GB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0493d70",
   "metadata": {},
   "source": [
    "## Configuration\n",
    "\n",
    "The system is pre-configured for optimal Colab performance:\n",
    "- **Model**: Phi-3 Mini (3.8B parameters)\n",
    "- **Quantization**: 4-bit (maximum memory savings)\n",
    "- **Samples**: 3 (quick testing)\n",
    "- **Memory monitoring**: Enabled\n",
    "\n",
    "You can modify settings in `config.py` if needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f72e9462",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check current configuration\n",
    "from config import *\n",
    "print(\"Current Configuration:\")\n",
    "print(f\"- LLM Provider: {DEFAULT_LLM_PROVIDER}\")\n",
    "print(f\"- Model: {HUGGINGFACE_MODEL}\")\n",
    "print(f\"- Quantization: {HUGGINGFACE_QUANTIZATION}\")\n",
    "print(f\"- Dataset: {DEFAULT_DATASET}\")\n",
    "print(f\"- Samples: {NUM_SAMPLES_TO_RUN}\")\n",
    "print(f\"- Compression: {DEFAULT_COMPRESSION_METHOD}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c04d456e",
   "metadata": {},
   "source": [
    "## Run Benchmark\n",
    "\n",
    "Execute the full benchmark pipeline:\n",
    "1. **Compression Phase**: Compress prompts using LLMLingua-2\n",
    "2. **Evaluation Phase**: Test both original and compressed prompts\n",
    "3. **Analysis**: Compare performance and consistency\n",
    "\n",
    "**Expected runtime**: ~5-10 minutes for 3 samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79c02e88",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the benchmark\n",
    "!python main.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7347ca70",
   "metadata": {},
   "source": [
    "## Results Analysis\n",
    "\n",
    "The benchmark generates:\n",
    "- **CSV logs** in the `results/` folder\n",
    "- **Performance metrics** for original vs compressed prompts\n",
    "- **Memory usage reports**\n",
    "- **Answer consistency analysis**\n",
    "\n",
    "Let's examine the latest results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ff58b8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import glob\n",
    "\n",
    "# Find the latest results file\n",
    "result_files = glob.glob('results/*.csv')\n",
    "if result_files:\n",
    "    latest_file = max(result_files, key=lambda x: x)\n",
    "    print(f\"Loading latest results: {latest_file}\")\n",
    "    \n",
    "    df = pd.read_csv(latest_file)\n",
    "    print(f\"\\nResults shape: {df.shape}\")\n",
    "    print(\"\\nColumns:\")\n",
    "    for col in df.columns:\n",
    "        print(f\"  - {col}\")\n",
    "    \n",
    "    print(\"\\nSample results:\")\n",
    "    display(df.head())\n",
    "else:\n",
    "    print(\"No results files found. Run the benchmark first!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40f7b497",
   "metadata": {},
   "source": [
    "## Memory Optimization Tips\n",
    "\n",
    "### For Colab Free Tier:\n",
    "1. **Use 4-bit quantization** (already enabled)\n",
    "2. **Keep sample size small** (3-5 samples recommended)\n",
    "3. **Monitor memory usage** (automatic)\n",
    "4. **Clear memory regularly** (automatic)\n",
    "\n",
    "### If you run out of memory:\n",
    "- Reduce `NUM_SAMPLES_TO_RUN` in `config.py`\n",
    "- Use an even smaller model like `microsoft/phi-2`\n",
    "- Restart the runtime and try again\n",
    "\n",
    "### For faster execution:\n",
    "- The system automatically uses the best available device (GPU > CPU)\n",
    "- Memory is cleared aggressively between operations\n",
    "- Progress bars show real-time status"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d42bc229",
   "metadata": {},
   "source": [
    "## Troubleshooting\n",
    "\n",
    "### Common Issues:\n",
    "\n",
    "**CUDA out of memory**\n",
    "```python\n",
    "# In config.py, try these settings:\n",
    "HUGGINGFACE_QUANTIZATION = \"4bit\"\n",
    "NUM_SAMPLES_TO_RUN = 2\n",
    "```\n",
    "\n",
    "**Model download slow**\n",
    "- Models are cached after first download\n",
    "- Use smaller models for testing\n",
    "\n",
    "**Dataset loading fails**\n",
    "- The system falls back to mock data automatically\n",
    "- No external dependencies required\n",
    "\n",
    "---\n",
    "\n",
    "## Next Steps\n",
    "\n",
    "1. **Experiment with different models** in `config.py`\n",
    "2. **Try different compression methods**\n",
    "3. **Analyze results** in the CSV files\n",
    "4. **Customize evaluation metrics**\n",
    "\n",
    "Happy benchmarking! ðŸŽ¯"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
